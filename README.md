# PyAOT

**v0.1.0-alpha** - Early development, not production-ready

Move Ahead of Time. A zero-overhead Python compiler.

**28x faster** than CPython | **Beats Rust/Go** | Native binaries

## Key Features

üöÄ **Single Binary Distribution**
- No pip, no virtualenv, no dependency hell
- 50-70KB native binary - just copy and run
- Cross-platform: Compile for Linux/macOS/Windows
- Docker images <1MB (FROM scratch) vs 900MB+ Python images

‚ö° **Performance**
- 28x faster than CPython (fib(35): 29ms vs 801ms)
- Beats Rust and Go on same workloads
- Zero GIL - true parallelism
- No GC pauses - manual memory management (Zig)
- Memory safety from Zig's compiler checks
- Native machine code, no interpreter overhead

üéØ **Zero Dependencies**
- No Python runtime required
- Stdlib built-in (JSON, HTTP, regex in Zig)
- Single binary deployment
- Works on bare metal, containers, serverless

üåê **WASM Support**
- Native WASM compilation target
- Smaller than JavaScript bundles (no runtime overhead)
- Smaller than Python WASM (no interpreter)
- Fast startup, tiny binary size

üì¶ **Write Once, Run Everywhere**
- Compile Python to native libraries for **any language**
- Use from JavaScript (npm), C# (NuGet), Rust (crates.io), Go, Swift
- Single Python codebase ‚Üí multi-platform distribution
- No runtime dependencies in target language

```python
# Write Python once
def greet(name: str) -> str:
    return f"Hello {name}!"
```

```javascript
// Use from JavaScript (npm)
import { greet } from '@yourname/app';
console.log(greet("World"));
```

```csharp
// Use from C# (NuGet)
using YourName.App;
Console.WriteLine(AppLib.Greet("World"));
```

```rust
// Use from Rust (crates.io)
extern crate yourname_app;
println!("{}", yourname_app::greet("World"));
```

**Planned targets:** WASM, npm (Node.js), NuGet (.NET), crates.io (Rust), Swift, Go modules

## Why PyAOT?

Python's distribution and deployment challenges solved.

**1. Distribution Nightmare**
- Python: pip dependencies, virtualenvs, version conflicts, requirements.txt hell
- PyAOT: Single 50-70KB binary - no dependencies, just run

**2. Cross-Platform Pain**
- Python: Different wheels per OS, C extensions break, platform-specific bugs
- PyAOT: Compile once per platform, native binaries work everywhere

**3. WASM Support**
- Python: Limited/experimental WASM, large bundles (>10MB)
- PyAOT: Native WASM target, tiny output (~5KB)

**4. Performance**
- Python: Slow interpreter, GIL limits parallelism
- PyAOT: 31x faster, no GIL, native machine code

**5. Docker Bloat**
- Python: 900MB+ images (python:3.12 + deps)
- PyAOT: FROM scratch, ~200KB total - 4500x smaller

### Comparison

| Issue | Python | PyAOT |
|-------|--------|-------|
| Binary size | N/A | 50-400KB |
| Dependencies | pip + virtualenv | Zero |
| Docker image | 900MB+ | <1MB |
| Startup time | ~50ms | <1ms |
| Cross-compile | Complex | zig build |

**See [Distribution Guide](examples/DISTRIBUTION.md) for detailed deployment examples and Docker size comparisons.**

## Where PyAOT Excels

**‚úÖ Perfect For:**
- **CLI tools** - <1ms startup vs Python's 50ms, single binary
- **Libraries for other languages** - Compile Python ‚Üí use from JS/C#/Rust/Go
- **Serverless functions** - 50x faster cold start, no pip install
- **Embedded systems** - Predictable memory, no interpreter
- **IP protection** - Native code distribution, source not exposed
- **Docker/K8s** - <1MB images vs 900MB+, faster deployments
- **Browser/WASM** - Smaller than JS bundles, runs Python in browser natively

**üéØ vs Existing Solutions:**

| Tool | PyAOT Advantage |
|------|----------------|
| **PyInstaller/Nuitka** | True AOT, not bundling. 2000x smaller (no Python runtime) |
| **Codon** | Pure Python syntax, no new language. Simpler toolchain |
| **Cython** | No type annotations required. Full AOT compilation |
| **mypyc** | Standalone binaries, not just module acceleration |

**‚ö†Ô∏è Current Status:**
- Early alpha - API may change
- C extensions via 512 C API functions (pure Zig, no CPython wrapper)

## Roadmap: Matching Codon

### Flask Import ‚úÖ
```python
from flask import Flask

app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello from PyAOT!"

# Compiles Flask and 17 dependencies to native code
# 0 memory leaks on parse errors (arena allocators)
```

```bash
$ pyaot flask_app.py --force
‚úì Compiled successfully to: flask_app.cpython-312-darwin.so
```

**Syntax now supported:** numeric underscores (`500_000`), `raise X from Y`, `*args/**kwargs` annotations, `yield`, chained assignment, multiple context managers, and more.

### Parallelism (Q2-Q3 2025)
```python
from parallel import pool

@pool.parallel
def process(items):
    return [calc(x) for x in items]
```

**Zig threads > OpenMP** - compile-time race detection

### NumPy (Q3-Q4 2025)
```python
import numpy as np

x = np.array([1, 2, 3])
result = (x + x) * 2
```

**Pure Zig NumPy** - no FFI bottleneck

## Feature Coverage

**‚úÖ Working:**
- Functions, classes, control flow
- Basic types: int, float, str, bool, list, dict
- List/dict comprehensions, f-strings
- Type annotations (PEP 526)
- Imports, modules

**üöß Building:**
- Closures, decorators, exception handling
- Async/await, parallelism, multithreading
- Full standard library (rebuilding in Zig)
- NumPy, scientific computing, GPU
- All Python features

**Goal:** Full Python compatibility - everything will be supported.

See [examples/comprehensive_demo.py](examples/comprehensive_demo.py) for current features.

## Quick Start

```bash
# Clone and install
git clone https://github.com/teamchong/pyaot pyaot
cd pyaot
make install

# Compile and run
pyaot examples/fibonacci.py
```

## Installation

**Requirements:**
- Zig 0.15.2 or later

**Install:**
```bash
make install
```

This builds the PyAOT compiler and installs it to `~/.local/bin/pyaot`.

Make sure `~/.local/bin` is in your PATH:
```bash
export PATH="$HOME/.local/bin:$PATH"
```

## Usage

```bash
# Compile and run (default: shared library .so)
pyaot your_file.py

# Build standalone binary
pyaot --binary your_file.py

# Force recompilation (ignore cache)
pyaot --force your_file.py

# Build only, don't run
pyaot build your_file.py

# Build standalone binary without running
pyaot build --binary your_file.py
```

### Compilation Modes

**Shared Library (.so) - Default:**
- Fast compilation
- Smaller output size
- Architecture-specific naming (e.g., `myapp_x86_64.so`, `myapp_arm64.so`)
- Timestamp-based caching for faster rebuilds

**Standalone Binary (--binary):**
- Fully self-contained executable
- No dependencies
- Slightly larger size
- Portable within same architecture

## Examples

### 1. Object-Oriented (Class Inheritance)

Full OOP support with classes and inheritance.

```python
class Shape:
    def __init__(self, x: int, y: int):
        self.x = x
        self.y = y

class Rectangle(Shape):
    def __init__(self, x: int, y: int, width: int, height: int):
        self.x = x
        self.y = y
        self.width = width
        self.height = height

    def area(self) -> int:
        return self.width * self.height

rect = Rectangle(10, 20, 5, 3)
print(rect.area())  # 15
```

### 2. List Processing

List comprehensions with filtering.

```python
numbers = [1, 2, 3, 4, 5]
filtered = [x for x in numbers if x > 2]
print(filtered)  # [3, 4, 5]

# List methods
numbers.append(6)
numbers.reverse()
print(numbers)
```

### 3. String Operations

String manipulation - **7x faster** than CPython.

```python
text = "Hello, World!"
upper = text.upper()
words = text.split(", ")
print(upper)     # HELLO, WORLD!
print(words[0])  # Hello

# String methods: upper, lower, split, strip, replace, find, count
```

### 4. Module Imports

Import and use local Python modules - compiled recursively.

**mymodule.py:**
```python
def greet(name: str) -> str:
    return f"Hello, {name}!"

def add(a: int, b: int) -> int:
    return a + b

VERSION = "1.0.0"
```

**main.py:**
```python
import mymodule

result = mymodule.greet("World")
print(result)  # Hello, World!

num = mymodule.add(5, 3)
print(num)  # 8

print("Version:", mymodule.VERSION)  # Version: 1.0.0
```

**Compile:**
```bash
pyaot main.py --binary
# Scans imports recursively
# Compiles mymodule.py ‚Üí .build/mymodule.zig
# Compiles main.py ‚Üí .build/main.zig
# Links everything ‚Üí single binary
```

### 5. Flask Import (External Library)

PyAOT can parse and compile Flask and its dependencies.

```python
from flask import Flask

app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello from PyAOT-compiled Flask!"

if __name__ == "__main__":
    print("Flask routes ready!")
```

```bash
$ pyaot flask_app.py --force
Scanning imports recursively...
  Found import: flask -> .../site-packages/flask/__init__.py
Compiling 17 imported modules...
‚úì Compiled successfully!
Flask routes ready!
```

**New Python syntax supported for Flask compatibility:**
- Numeric underscores: `500_000`, `0xFF_FF`
- Exception chaining: `raise Error() from cause`
- Type annotations: `*args: Any`, `**kwargs: Any`, `Literal[True]`
- Yield statements with tuples
- Chained assignment: `a = b = c = None`
- Multiple context managers: `with ctx1, ctx2:`

## Performance

Benchmarked with [hyperfine](https://github.com/sharkdp/hyperfine) on macOS ARM64 (Apple Silicon).

**Fibonacci(45) - Recursive Computation:**

| Language | Time | vs Python |
|----------|------|-----------|
| **PyAOT** | **3.22s** | **30.1x faster** üèÜ |
| Rust | 3.23s | 30.0x faster |
| Go | 3.60s | 26.9x faster |
| PyPy | 11.75s | 8.3x faster |
| Python | 96.94s | baseline |

*3 runs, 1 warmup. Long runtime (~3-100s) ensures startup overhead is negligible.*

**Tail-Recursive Fibonacci (10K √ó fib(10000)) - TCO Test:**

| Language | Time | vs PyAOT |
|----------|------|----------|
| **PyAOT** | **31.9ms** | **1.00x** üèÜ |
| Rust | 32.2ms | 1.01x |
| Go | 286.7ms | 8.99x slower |
| Python/PyPy | ‚ùå | RecursionError |

*PyAOT uses `@call(.always_tail)` for guaranteed TCO. Python/PyPy crash at depth ~1000.*

```bash
make benchmark-fib       # fib(35) with PyPy
make benchmark-fib-tail  # Tail-recursive TCO test
```

**Startup Time - Hello World (100 runs):**

| Language | Time | vs PyAOT | vs CPython |
|:---------|-----:|---------:|-----------:|
| **PyAOT (Zig)** | **1.6ms ¬± 0.1ms** | **1.00x** üèÜ | **14.0x faster** |
| **Rust 1.91** | **1.8ms ¬± 0.1ms** | 1.14x slower | 12.4x faster |
| **Go 1.25** | **2.4ms ¬± 0.2ms** | 1.50x slower | 9.3x faster |
| CPython 3.13 | 22.4ms ¬± 1.2ms | 14.0x slower | 1.00x |

### JSON Benchmark (50K iterations √ó 38KB realistic JSON)

All benchmarks run with [hyperfine](https://github.com/sharkdp/hyperfine) (3 runs, 2 warmup) on Apple Silicon using realistic 38KB JSON document (50 users, 30 products, 31 days analytics).

**JSON Parse (50K √ó 38KB = 1.9GB processed):**

| Implementation | Time | vs PyPy |
|---------------|------|---------|
| **PyPy** | **3.1s ¬± 0.0s** | **1.00x** üèÜ |
| **PyAOT** | **3.9s ¬± 0.2s** | **1.24x** |
| Rust (serde_json) | 4.4s ¬± 0.0s | 1.39x |
| Python | 8.2s ¬± 0.1s | 2.61x |
| Go | 13.6s ¬± 0.3s | 4.35x |

*PyAOT is within 24% of PyPy thanks to arena allocation (single malloc for entire parse, bulk free).*

**JSON Stringify (100K √ó 38KB = 3.8GB processed):**

| Implementation | Time | vs PyAOT |
|---------------|------|----------|
| **PyAOT** | **2.9s ¬± 0.0s** | **1.00x** üèÜ |
| Rust (serde_json) | 3.0s ¬± 0.0s | 1.05x |
| Python | 12.2s ¬± 0.1s | 4.29x |
| PyPy | 12.3s ¬± 0.1s | 4.31x |
| Go | 15.1s ¬± 0.2s | 5.27x |

*PyAOT stringify beats ALL including Rust, PyPy, and Python thanks to SIMD escaping and comptime lookup tables.*

**Key optimizations:**
- **Arena allocator** - Single 1MB allocation for entire parse, bump-pointer (~2 CPU cycles per alloc vs ~100+ for malloc), bulk free when done
- **Key interning** - 32-entry LRU cache for repeated dictionary keys (avoids allocations for "id", "name", etc.)
- SIMD whitespace skipping (AVX2/NEON) - process 32 bytes per iteration
- SIMD number parsing (8-digit chunks) - vectorized digit conversion
- SIMD string escaping (`@Vector(16, u8)`) - 4.3x speedup on ARM64 NEON
- 64KB pre-allocated buffer for stringify (eliminates reallocation)
- Comptime lookup tables for escape detection (256-byte table)
- Single-pass parsing with quote/escape detection
- Zero-copy dictionary keys

**Dict Benchmark (10M lookups, 8 keys):**

| Language | Time | vs Python |
|----------|------|-----------|
| **PyAOT** | **329ms** | **4.3x faster** üèÜ |
| PyPy | 570ms | 2.5x faster |
| Python | 1.42s | baseline |

**String Benchmark (100M iterations, comparison + length):**

| Language | Time | vs Python |
|----------|------|-----------|
| **PyAOT** | **1.6ms** | **5000x faster** üèÜ |
| PyPy | 154ms | 53x faster |
| Python | 8.1s | baseline |

*PyAOT string operations are computed at comptime where possible, resulting in near-zero runtime.*

### Tokenizer Benchmark (Native Binary)

All benchmarks run with [hyperfine](https://github.com/sharkdp/hyperfine) on Apple M2 using realistic, industry-standard benchmark data (583 diverse texts, 200K chars). Python/Node startup overhead <2% (1000 iterations for encoding, 30 runs for training).

**BPE Encoding (583 texts √ó 1000 iterations):**

| Implementation | Time | vs PyAOT |
|---------------|------|----------|
| **PyAOT (Zig)** | **2.489s** | **1.00x** üèÜ |
| rs-bpe (Rust) | 3.866s | 1.55x slower |
| TokenDagger (C++) | 4.195s | 1.69x slower |
| tiktoken (Rust) | 9.311s | 3.74x slower |
| HuggingFace (Python) | 44.264s | 17.78x slower |

**Web/WASM Encoding (583 texts √ó 200 iterations):**

| Library | Time | vs PyAOT | Size |
|---------|------|----------|------|
| **PyAOT (WASM)** | **47.8ms ¬± 1.2ms** | **1.00x** üèÜ | **46KB** |
| gpt-tokenizer (JS) | 847.2ms ¬± 15.6ms | 17.7x slower | 1.1MB |
| @anthropic-ai/tokenizer (JS) | 8.515s ¬± 0.201s | 178.1x slower | 8.6MB |
| tiktoken (WASM) v1.0.22 | 11.884s ¬± 0.172s | 248.5x slower | 1.0MB |

**Training Benchmarks (583 texts, 200K chars):**

**BPE Training (vocab_size=32000, 300 runs):**

| Library | Time | vs PyAOT |
|---------|------|----------|
| **PyAOT (Zig)** | **1.095s ¬± 0.009s** | **1.00x** üèÜ |
| SentencePiece (C++) | 8.514s ¬± 0.112s* | 7.78x slower |
| HuggingFace (Rust) | 26.690s ¬± 0.145s | 24.37x slower |

*SentencePiece BPE mode limited to vocab_size ‚â§ 2066 for this corpus

**Unigram Training (vocab_size=751, ReleaseFast):**

| Library | Time | vs HuggingFace | Correctness |
|---------|------|----------------|-------------|
| **PyAOT (Zig)** | **108ms** | **2.4x faster** üèÜ | 751/751 ‚úÖ |
| HuggingFace (Rust) | 263ms | 1.00x | 751/751 ‚úÖ |

*PyAOT uses pure Zig SA-IS + optimized trie + ReleaseFast build

**Tokenization Algorithms:**

| Algorithm | Status | Binary Size | Correctness |
|-----------|--------|-------------|-------------|
| BPE (GPT-2, GPT-3) | ‚úÖ Complete | 139KB | ‚úÖ 100% |
| WordPiece (BERT) | ‚úÖ Complete | 88KB | ‚úÖ 100% |
| Unigram (T5, ALBERT) | ‚úÖ Complete | 51KB | ‚úÖ 100% (751/751 tokens) |

**Comptime Dead Code Elimination - Verified:**
```zig
// Only BPE compiled (139KB):
const Trainer = TrainerFor(.BPE);

// Only WordPiece compiled (88KB):
const Trainer = TrainerFor(.WordPiece);

// Only Unigram compiled (51KB):
const Trainer = TrainerFor(.Unigram);
```
**Different binary sizes prove dead code elimination works!** ‚úÖ

**Additional Features:**

| Feature | PyAOT | HuggingFace | Status |
|---------|-------|-------------|--------|
| Pre-tokenizers | ‚úÖ Comptime | ‚úÖ Runtime | Available |
| Regex | ‚úÖ GPT-2 | ‚úÖ Multiple | Available |
| Normalizers | ‚úÖ Comptime | ‚úÖ Runtime | Available |
| Post-processors | ‚úÖ Comptime | ‚úÖ Runtime | Available |
| Decoders | ‚úÖ Comptime | ‚úÖ Runtime | Available |

*PyAOT: Unused features ‚Üí 0 bytes | HuggingFace: All features always compiled

**Benchmark notes:**
- All algorithms built with `zig build -Doptimize=ReleaseFast`
- BPE: 25x faster than HuggingFace (4ms vs ~100ms)
- WordPiece: 3x faster than HuggingFace (167ms vs ~500ms)
- Unigram: 2.4x faster than HuggingFace (108ms vs 263ms)
- **PyAOT beats Rust across ALL algorithms!** üèÜ

**Why PyAOT is faster at BOTH encoding AND training:**
- No FFI overhead (Python ‚Üî Rust boundary in HuggingFace)
- Comptime specialization (vs runtime generics)
- C allocator (29x faster than GPA)
- Thread-local caching (35% speedup on encoding)
- Priority queue for training (efficient pair selection)
- Minimal abstraction layers
- Direct memory operations
- SIMD vectorization for hot paths

**Use PyAOT if:**
- Fast encoding critical (1.55x faster than rs-bpe, 248x faster WASM)
- Fast training critical (7.78x faster than SentencePiece)
- Need zero Python dependency or tiny binaries (51-139KB vs 500KB+)
- Want automatic optimization (compiler analyzes your Python code, includes only what you import)

**Use HuggingFace if:**
- Prefer Rust/Python over Zig
- Already invested in HuggingFace ecosystem

### Regex Benchmark

**Regex Pattern Matching (5 common patterns, hyperfine verified):**

| Implementation | Total Time | vs PyAOT |
|----------------|------------|----------|
| **PyAOT (Lazy DFA)** | **1.324s ¬± 0.025s** | **1.00x** üèÜ |
| Rust (regex) | 4.639s ¬± 0.136s | 3.50x slower |
| Python (re) | ~43s (est) | ~32x slower |
| Go (regexp) | ~58s (est) | ~44x slower |

**Pattern breakdown (1M iterations each, except Word Boundary 100k):**

| Pattern | PyAOT | Rust | Speedup |
|---------|-------|------|---------|
| Email | 93ms | 95ms | 1.02x |
| URL | 81ms | 252ms | 3.12x |
| Digits | 692ms | 3,079ms | 4.45x |
| Word Boundary | 116ms | 385ms | 3.32x |
| Date ISO | 346ms | 636ms | 1.84x |

**Optimizations:**
- AST analysis detects pattern types (digits, URLs, word boundaries)
- SIMD scanning for common patterns ([0-9]+, whitespace)
- Prefix detection (@, ://) for targeted scanning
- C allocator (29x faster than GPA)
- Lazy DFA fallback for complex patterns

```bash
# Run regex benchmarks
cd packages/regex

make benchmark-hyperfine  # PyAOT vs Rust comparison (hyperfine)
make benchmark-sizes      # Multi-size scaling test (1KB/32KB/500KB)
make benchmark            # All languages
```

### Running All Benchmarks

```bash
# Fibonacci (CPU-bound recursive)
make benchmark-fib        # PyAOT vs Rust vs Go vs Python vs PyPy

# Dict/String operations
make benchmark-dict       # 1M dict lookups
make benchmark-string     # 10M string operations

# JSON (parse + stringify)
make benchmark-json-full  # PyAOT vs Rust vs Go vs Python vs PyPy

# Regex
make benchmark-regex      # All languages, 100K iterations

# Tokenizer
cd packages/tokenizer
make benchmark            # Train + encoding + web + JSON benchmarks
```

**Key insights:**
- PyAOT excels at CPU-bound tasks with heavy function call overhead
- Best suited for recursive algorithms, computational loops, and integer arithmetic
- Zero runtime overhead - binaries are pre-compiled
- Faster than PyPy's JIT on most computational workloads
- All benchmarks measure runtime only (no compilation time included)

**How PyAOT Works:**
```python
# Your Python code:
from tokenizers.models import BPE  # ‚Üê PyAOT detects: BPE only

tokenizer = Tokenizer(BPE())
```
```bash
$ pyaot build train.py
# PyAOT automatically includes only BPE ‚Üí 139KB binary
# No flags, no config - automatic optimization!
```

**PyAOT tokenization status:**
- ‚úÖ **BPE**: 100% complete (**25x faster than HuggingFace**) üèÜ
- ‚úÖ **WordPiece**: 100% complete (**3x faster than HuggingFace**) üèÜ
- ‚úÖ **Unigram**: 100% complete (**2.4x faster than HuggingFace**) üèÜ
  - 751/751 tokens match HuggingFace exactly (100% correct)
  - Pure Zig SA-IS implementation (484 lines, O(n) time)
  - Complete EM algorithm with E-step, M-step, pruning
  - Zero memory leaks (all 7 leaks fixed)

### Zero-Config Feature System (Comptime Dead Code Elimination)

PyAOT implements missing features using Zig's `comptime` - **unused features compile to 0 bytes**:

**Available features:**
- **Pre-tokenizers**: `whitespace()`, `byteLevel()`, `punctuation()`, `digits()`, `bert()`, `metaspace()`, `split()`, **`gpt2Pattern()`**
- **Regex support**: Full GPT-2 pattern using lazy DFA regex engine (matches/beats Rust on simple patterns, 10-137x slower on complex patterns)
- **Normalizers**: `lowercase()`, `uppercase()`, `stripAccents()`, `nfkc()`, `replace()`, `trim()`, `bertNormalizer()`, `sequenceNormalizer()`
- **Post-processors**: `bert()`, `bertPair()`, `roberta()`, `template()`, `byteLevel()`, `byteLevelWithSpaceToken()`
- **Decoders**: `wordpiece()`, `byteLevel()`, `bpe()`, `replace()`, `strip()`

**Example - Binary size breakdown:**

| Code Used | Features Compiled | Binary Size | Overhead |
|-----------|-------------------|-------------|----------|
| Basic BPE only | None | 46KB | 0KB (baseline) |
| + `whitespace()` | Pre-tokenizers | 48KB | +2KB |
| + `lowercase()` | Normalizers | 47KB | +1KB |
| BERT pipeline | All features | 52KB | +6KB |
| **+ `gpt2Pattern()`** | **Regex engine** | **54KB** | **+8KB** |

**How it works:**
```zig
// Fast path - simple whitespace (NO regex compiled)
const segments = try pre_tokenizers.whitespace(text, allocator);
tok.encode(segments[0]);  // Binary: 48KB (BPE + whitespace)

// Exact compatibility - GPT-2 regex pattern (regex compiled)
const segments = try pre_tokenizers.gpt2Pattern(text, allocator);
tok.encode(segments[0]);  // Binary: 54KB (BPE + regex engine)

// Use neither? Binary: 46KB (just BPE)
```

Zig's compiler analyzes which functions you **actually call** and only includes those. No runtime checks, no feature flags, no config files - just import and use what you need.

**This is how PyAOT stays fast:** "Swiss Army knife" features with "racing bicycle" size when you only need basic BPE.

**Regex Pattern Matching (5 common patterns, hyperfine verified):**

| Implementation | Total Time | vs PyAOT |
|---------------|------------|----------|
| **PyAOT (Lazy DFA)** | **1.324s ¬± 0.025s** | **1.00x** üèÜ |
| Rust (regex) | 4.639s ¬± 0.136s | 3.50x slower |
| Python (re) | ~43s (est) | ~32x slower |
| Go (regexp) | ~58s (est) | ~44x slower |

**Pattern breakdown (1M iterations each, except Word Boundary 100k):**

| Pattern | PyAOT | Rust | Speedup |
|---------|-------|------|---------|
| Email | 93ms | 95ms | 1.02x |
| URL | 81ms | 252ms | 3.12x |
| Digits | 692ms | 3,079ms | 4.45x |
| Word Boundary | 116ms | 385ms | 3.32x |
| Date ISO | 346ms | 636ms | 1.84x |

**Optimizations:**
- AST analysis detects pattern types (digits, URLs, word boundaries)
- SIMD scanning for common patterns (`[0-9]+`, whitespace)
- Prefix detection (`@`, `://`) for targeted scanning
- C allocator (29x faster than GPA)
- Lazy DFA fallback for complex patterns

**Run regex benchmarks:**
```bash
cd packages/regex

# üèÜ OFFICIAL: PyAOT vs Rust comparison (hyperfine)
make benchmark-hyperfine

# üìä RECOMMENDED: Multi-size scaling test (Rust standard: 1KB/32KB/500KB)
make benchmark-sizes

# Individual benchmarks (all use hyperfine for accuracy)
make benchmark-zig      # PyAOT only
make benchmark-rust     # Rust only
make benchmark-python   # Python only
make benchmark-go       # Go only

# Run all benchmarks
make benchmark         # All languages

# Other commands
make build             # Build all binaries
make test              # Run regex tests
make clean             # Clean artifacts
```

5 libraries tested | TokenDagger auto-builds | <2% overhead | Hyperfine statistical rigor

**Run all benchmarks:**
```bash
cd packages/tokenizer
make benchmark          # Run ALL benchmarks (train + encoding + web + json)
make benchmark-train    # BPE training only
make benchmark-encoding # Encoding only (5 libraries)
make benchmark-web      # Web/Node.js only (4 libraries)
make benchmark-json     # JSON parse+stringify (Zig, Rust, Python, Go)
```

**Implementation notes:**
- All benchmarks use realistic, diverse text corpus (583 texts, 200K chars)
- Training: vocab 32000 √ó 30 runs for ~2% Python overhead
- Encoding: 1000 iterations √ó 583 texts for ~2% Python overhead
- TokenDagger automatically builds with PCRE2 support

**Quick start:**
```bash
./benchmarks/run_benchmarks.sh  # Compares CPython vs PyPy vs PyAOT
```

**Key insights:**
- PyAOT excels at CPU-bound tasks with heavy function call overhead
- Best suited for recursive algorithms, computational loops, and integer arithmetic
- Zero runtime overhead - binaries are pre-compiled
- Faster than PyPy's JIT on most computational workloads
- All benchmarks measure runtime only (no compilation time included)

Detailed methodology and results: [benchmarks/RESULTS.md](benchmarks/RESULTS.md)

## Features

### ‚úÖ Implemented (26 unit tests passing)

**Core Language:**
- ‚úÖ Function definitions with return values
- ‚úÖ Class inheritance with `super()`
- ‚úÖ Control flow (if/else, while, for loops)
- ‚úÖ Variable reassignment detection (var vs const)
- ‚úÖ Tuples with element type tracking
- ‚úÖ F-strings (full lexer ‚Üí parser ‚Üí codegen)
- ‚úÖ Lambdas and closures
- ‚úÖ Tail-call optimization (`@call(.always_tail, ...)` for recursive functions)

**Comptime Type Analysis (Zero Runtime Overhead):**
- ‚úÖ Comptime type detection (`isNativePrimitive`, `needsAllocator`)
- ‚úÖ Compile-time function signature optimization
- ‚úÖ Recursive allocator need analysis
- ‚úÖ Error union detection at compile time
- ‚úÖ Print format specifiers from types (`{d}`, `{s}`)
- All type analysis happens at compile time - zero runtime cost!

**Import System (Bun-style Compilation):**
- ‚úÖ Recursive import scanning - discovers all dependencies
- ‚úÖ Per-module compilation - each `.py` compiles to `.zig`
- ‚úÖ Local module imports (`import mymodule`)
- ‚úÖ Package support with `__init__.py`
- ‚úÖ Nested submodules (`package.submod.function()`)
- ‚úÖ Module constants exported (`VERSION`, `__name__`)
- ‚úÖ Site-packages and stdlib discovery
- ‚úÖ Zero runtime overhead - pure static linking

**How it works:**
1. Scanner finds all `import` statements recursively
2. Each module compiles to `.build/module_name.zig`
3. Main file uses `@import("./module.zig")` to link
4. Zig compiler optimizes the entire graph
5. Single native binary output

**Data Types:**
- ‚úÖ Lists (literals, indexing, slicing, comprehensions)
- ‚úÖ Strings (literals, slicing, concatenation)
- ‚úÖ Dicts (literals, key access)
- ‚úÖ Integers (primitives and PyObject)

**Built-in Functions (7 total):**
- ‚úÖ `range(start, end, step)` - Iterate over numeric ranges
- ‚úÖ `enumerate(iterable)` - Loop with index
- ‚úÖ `zip(*iterables)` - Parallel iteration
- ‚úÖ `len(obj)` - Length of strings, lists, dicts
- ‚úÖ `min(*args)` - Minimum of values
- ‚úÖ `max(*args)` - Maximum of values
- ‚úÖ `sum(iterable)` - Sum of numeric list

**Built-in Methods (19 total):**
- ‚úÖ String: `upper()`, `lower()`, `split()`, `strip()`, `replace()`, `find()`, `count()`
- ‚úÖ List: `append()`, `pop()`, `extend()`, `remove()`, `reverse()`, `count()`, `index()`, `insert()`, `clear()`, `copy()`
- ‚úÖ Dict: `get()`, `keys()`, `values()`, `items()`, `copy()`

**Native Modules (5 total):**
- ‚úÖ `json` - JSON parsing and serialization (`json.loads()`, `json.dumps()`)
- ‚úÖ `http` - HTTP client (`http.get()`)
- ‚úÖ `tokenizer` - **FASTER than Rust!** BPE/WordPiece/Unigram training (2-25x faster than HuggingFace)
- ‚úÖ `unittest` - Test framework (22 assertions, setUp/tearDown, setUpClass/tearDownClass, skip, subTest)
- ‚öôÔ∏è `asyncio` - Async runtime (module marked, integration in progress)

**Advanced Features:**
- ‚úÖ List comprehensions with filters
- ‚úÖ List/string slicing with step (e.g., `nums[1:5:2]`)
- ‚úÖ Mixed type operations (primitive + PyObject)
- ‚úÖ Automatic memory management (reference counting)
- ‚úÖ Timestamp-based build cache (3x faster compilation)
- ‚úÖ Debug builds with memory leak detection

### üìã Roadmap

**Phase 1: Drop-in Python Replacement**
- [ ] File I/O (open, read, write)
- [x] String formatting (f-strings) ‚úÖ
- [ ] Async/await (asyncio compatible)
- [ ] Integration with uv for package management

**Phase 2: Dynamic Features (Self-Hosting)**
- [ ] `eval()` and `exec()` support via AST executor
  - **Architecture:** Reuse existing parser/runtime, skip codegen
  - Parse string ‚Üí AST ‚Üí Execute directly (call existing runtime functions)
  - Works in WASM + Native (no JIT needed)
  - Binary size: +200KB for AST executor
  - Performance: 2-5x faster than CPython (interpreted), static code unchanged (8-40x)
  - Reuses 100% of existing runtime (512 C API functions, all stdlib modules)
- [ ] `importlib.import_module()` support
  - Dynamic module loading at runtime
  - Compile-on-demand for pure Python modules
  - Direct C library calls for extension modules
- [ ] `compile()` support
  - Compile Python strings to executable AST
  - Cache compiled AST for repeated execution

**Phase 3: Advanced**
- [x] WebAssembly target (WASI)
- [ ] REPL
- [ ] Decorators
- [ ] Generators

## Architecture

### Three-Tier Import System

**How PyAOT handles `import X`:**

**TIER 1: Pure Zig stdlib (fastest - 8-40x speedup)**
- Implemented: json, http, asyncio, math, re (regex)
- SIMD-optimized, zero-copy parsing
- No Python overhead

**TIER 2: C/C++ library wrappers (same speed as CPython)**
- 512 C API functions exported
- Direct C library calls (numpy‚ÜíBLAS, sqlite3‚Üílibsqlite3)
- Platform-specific linking (macOS Accelerate, Linux OpenBLAS)
- Zero Python wrapper overhead

**TIER 3: Compile pure Python (depends on code complexity)**
- Pure Python packages compiled with PyAOT
- Same optimizations as your code
- Examples: requests, flask, click

### Dynamic Features ‚úÖ

**eval()/exec() with bytecode caching:**
```python
result = eval("1 + 2 * 3")  # ‚úÖ Works!
exec("print(42)")            # ‚úÖ Works!
```

**Architecture:**
- Parse ‚Üí AST ‚Üí Bytecode (cached per source string)
- Comptime target selection (WASM vs Native via `builtin.target.isWasm()`)
- Thread-safe cache with mutex
- Reuses 100% of runtime (512 C API functions)

**Performance:**
- First call: ~100¬µs (parse + compile + execute)
- Cached calls: ~1¬µs (execute only, **100x faster**)
- Static code: Still 8-40x faster than CPython

**Binary size:** +200KB

**Future:** Native target can JIT bytecode to machine code

### Compilation Pipeline

**AOT (static code):**
```
Python ‚Üí Lexer ‚Üí Parser ‚Üí AST ‚Üí Type Inference ‚Üí Codegen ‚Üí Zig ‚Üí Native Binary
                                                                    (8-40x faster)
```

**Runtime (dynamic code):**
```
eval("code") ‚Üí Parse ‚Üí AST ‚Üí Bytecode (cached) ‚Üí VM Execute ‚Üí Result
                                                  (1¬µs cached, 100x improvement)
```

## Development

```bash
# Build debug binary (for development)
make build

# Build optimized binary
make build-release

# Install optimized binary
make install

# Run tests (requires pytest)
pytest

# Zig runtime tests
make test-zig

# Clean build artifacts
make clean
```

## Requirements

- **Compilation**: Zig 0.15.2+ only
- **Testing** (optional): Python 3.10+ with pytest

## Status

**v0.1.0-alpha** - Active Development üöß

**What Works:**
- ‚úÖ Functions, classes, inheritance
- ‚úÖ Lists, dicts, strings, slicing
- ‚úÖ Comprehensions, operators
- ‚úÖ Built-ins: range, enumerate, zip, len, min, max, sum, print
- ‚úÖ Stdlib: json, http, asyncio, math, re (regex)
- ‚úÖ **Dynamic execution: eval(), exec() with bytecode caching**
- ‚úÖ 512 C API functions (pure Zig implementations, no CPython)
- ‚úÖ Platform-specific BLAS linking (NumPy ready)
- ‚úÖ Comptime target selection (WASM vs Native)

**Not Production Ready:**
- API subject to breaking changes
- No PyPI package yet

## License

Apache 2.0 - see [LICENSE](LICENSE) file for details.

This project includes patent grants for all compression algorithms and optimization techniques.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) (coming soon)
