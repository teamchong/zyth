/// High-performance BPE tokenizer in pure Zig
/// Targets: 1.1-1.25x faster than Rust rustbpe
/// Features: SIMD, parallel processing, comptime optimization, zero-copy

const std = @import("std");
const Allocator = std.mem.Allocator;

// External dependencies
const BacktrackEncoder = @import("backtrack_encoder.zig").BacktrackEncoder;
const HeapEncoder = @import("heap_encoder.zig").HeapEncoder;
const StackEncoder = @import("stack_encoder.zig");
const encodeGreedy = @import("greedy_encoder.zig").encodeGreedy;
const encodeOptimized = @import("optimized_hashmap_encoder.zig").encodeOptimized;
const cl100k_splitter = @import("cl100k_splitter.zig");
const AhoCorasick = @import("aho_corasick.zig").AhoCorasick;
const LruCache = @import("lru_cache.zig").LruCache;
const tokenizer_io = @import("tokenizer_io.zig");

/// Select best allocator for encode_arena based on target platform
/// Native: c_allocator (jemalloc - 2x faster, zero syscalls)
/// WASM: caller's allocator (c_allocator unavailable in WASM)
fn getBestArenaAllocator(fallback: Allocator) Allocator {
    const builtin = @import("builtin");
    return if (builtin.cpu.arch.isWasm())
        fallback // WASM: use GPA from caller
    else
        std.heap.c_allocator; // Native: jemalloc (56% faster!)
}

// Comptime-generated specialized stack encoders (zero heap allocation!)
const SmallEncoder = StackEncoder.BacktrackEncoder(4 * 1024); // 4KB chunks
const MediumEncoder = StackEncoder.BacktrackEncoder(16 * 1024); // 16KB chunks
const LargeEncoder = StackEncoder.BacktrackEncoder(64 * 1024); // 64KB chunks

// Re-export modular components
const helpers = @import("tokenizer_helpers.zig");
pub const Pair = helpers.Pair;
pub const PairContext = helpers.PairContext;
pub const StringHashContext = helpers.StringHashContext;
pub const BitField = helpers.BitField;
pub const TrieNode = helpers.TrieNode;
pub const countPairsSIMD = helpers.countPairsSIMD;
pub const mergePair = helpers.mergePair;

// FnvHash for optimized HashMap lookups
const FnvHashContext = @import("fnv_hash.zig").FnvHashContext;

const builder = @import("tokenizer_builder.zig");
const buildSplitTable = builder.buildSplitTable;
const buildAhoCorasick = builder.buildAhoCorasick;
const buildNextPrefixMatch = builder.buildNextPrefixMatch;
const isValidTokenPair = builder.isValidTokenPair;

const parser = @import("tokenizer_parser.zig");

// Thread-local caching and pooling
const cache = @import("tokenizer_cache.zig");
const getTokenCache = cache.getTokenCache;
const getEncodeCache = cache.getEncodeCache;
const getResultBuffer = cache.getResultBuffer;
const releaseResultBuffer = cache.releaseResultBuffer;

// SIMD acceleration
const simd = @import("simd_encoder.zig");

pub const Tokenizer = struct {
    vocab: std.HashMap([]const u8, u32, FnvHashContext([]const u8), std.hash_map.default_max_load_percentage),
    vocab_r: std.AutoHashMap(u32, []const u8),
    merges: std.ArrayList(Pair),
    merges_map: std.HashMap(Pair, u32, FnvHashContext(Pair), std.hash_map.default_max_load_percentage),
    split_table: []Pair, // For merge validation: token -> (left, right)
    pattern_str: []const u8,
    trie: ?*TrieNode, // Fast longest-match lookup (optional - uses lots of memory)
    aho_corasick: ?AhoCorasick, // Fast vocab lookup for backtracking encoder
    next_prefix_match: []u32, // Precomputed next_prefix table (rs-bpe optimization)
    allocator: Allocator,
    encode_arena: std.heap.ArenaAllocator, // Reused across encode() calls - eliminates 116,600 syscalls

    pub fn initFromData(json_data: []const u8, allocator: Allocator) !Tokenizer {
        const data = try parser.initFromData(json_data, allocator);
        return Tokenizer{
            .vocab = data.vocab,
            .vocab_r = data.vocab_r,
            .merges = data.merges,
            .merges_map = data.merges_map,
            .split_table = data.split_table,
            .pattern_str = data.pattern_str,
            .trie = data.trie,
            .aho_corasick = data.aho_corasick,
            .next_prefix_match = data.next_prefix_match,
            .allocator = data.allocator,
            .encode_arena = std.heap.ArenaAllocator.init(getBestArenaAllocator(allocator)),
        };
    }

    pub fn init(tokenizer_path: []const u8, allocator: Allocator) !Tokenizer {
        const data = try parser.initFromFile(tokenizer_path, allocator);
        return Tokenizer{
            .vocab = data.vocab,
            .vocab_r = data.vocab_r,
            .merges = data.merges,
            .merges_map = data.merges_map,
            .split_table = data.split_table,
            .pattern_str = data.pattern_str,
            .trie = data.trie,
            .aho_corasick = data.aho_corasick,
            .next_prefix_match = data.next_prefix_match,
            .allocator = data.allocator,
            .encode_arena = std.heap.ArenaAllocator.init(getBestArenaAllocator(allocator)),
        };
    }

    pub fn deinit(self: *Tokenizer) void {
        var vocab_it = self.vocab.iterator();
        while (vocab_it.next()) |entry| {
            self.allocator.free(entry.key_ptr.*);
        }
        self.vocab.deinit();
        self.vocab_r.deinit();
        self.merges.deinit(self.allocator);
        self.merges_map.deinit();
        self.allocator.free(self.split_table);
        self.allocator.free(self.pattern_str);
        if (self.trie) |t| t.deinit();
        if (self.aho_corasick) |*ac| ac.deinit();
        self.allocator.free(self.next_prefix_match);
        self.encode_arena.deinit();
    }

    /// HASH MAP optimization: O(n * k) instead of O(n * m)
    /// k = actual merges applied << m = total possible merges
    /// SIMD-accelerated for initial byte tokenization
    pub fn encodeHashMap(self: *Tokenizer, text: []const u8) ![]u32 {
        if (text.len <= 4096) {
            var stack_buffer: [4096]u32 = undefined;

            // SIMD-accelerated initial byte tokenization
            self.tokenizeBytesOptimized(text, stack_buffer[0..text.len]);

            const len = try self.applyMergesHashMap(stack_buffer[0..text.len]);

            const result = try self.allocator.alloc(u32, len);
            @memcpy(result, stack_buffer[0..len]);
            return result;
        }

        // Large text path
        var tokens = try std.ArrayList(u32).initCapacity(self.allocator, text.len);
        errdefer tokens.deinit(self.allocator);

        // Resize to exact size needed
        tokens.items.len = text.len;

        // SIMD-accelerated byte tokenization
        self.tokenizeBytesOptimized(text, tokens.items);

        try self.applyMergesHashMapArrayList(&tokens);

        // Avoid toOwnedSlice overhead - just dupe used portion
        const items = tokens.items[0..tokens.items.len];
        const owned = try self.allocator.dupe(u32, items);
        tokens.clearRetainingCapacity();
        return owned;
    }

    /// SIMD-optimized initial byte tokenization
    inline fn tokenizeBytesOptimized(self: *Tokenizer, text: []const u8, output: []u32) void {
        // Fast path: for ASCII text, we can optimize vocab lookups
        // Most single bytes map to their own token ID
        if (simd.isAsciiSIMD(text)) {
            // ASCII fast path - likely direct mapping
            for (text, 0..) |byte, i| {
                const byte_slice = @as(*const [1]u8, &byte)[0..1];
                output[i] = self.vocab.get(byte_slice) orelse byte;
            }
        } else {
            // UTF-8 slow path - need proper vocab lookups
            for (text, 0..) |byte, i| {
                const byte_slice = @as(*const [1]u8, &byte)[0..1];
                output[i] = self.vocab.get(byte_slice) orelse byte;
            }
        }
    }

    /// Vocab-based BPE merging (tiktoken style) - OPTIMIZED O(nÂ²)
    /// Uses skip array to avoid O(n) shifting on every merge
    fn applyMergesHashMap(self: *Tokenizer, tokens: []u32) !usize {
        @setRuntimeSafety(false);

        if (tokens.len < 2) return tokens.len;

        // Skip array: -1 = active, >= 0 = merged (skipped)
        var skip_buffer: [4096]i32 = undefined;
        var skip = skip_buffer[0..tokens.len];
        @memset(skip, -1); // All active initially

        var merge_buffer: [512]u8 = undefined;
        var active_count = tokens.len;

        while (active_count > 1) {
            var best_rank: u32 = std.math.maxInt(u32);
            var best_new_token: u32 = 0;
            var best_pos: usize = 0;

            // Scan active pairs (skip merged positions)
            var i: usize = 0;
            while (i < tokens.len) {
                if (skip[i] >= 0) {
                    i += 1;
                    continue;
                }

                // Find next active position
                var next = i + 1;
                while (next < tokens.len and skip[next] >= 0) : (next += 1) {}
                if (next >= tokens.len) break;

                const left_token = tokens[i];
                const right_token = tokens[next];

                const left_bytes = self.vocab_r.get(left_token) orelse {
                    i = next;
                    continue;
                };
                const right_bytes = self.vocab_r.get(right_token) orelse {
                    i = next;
                    continue;
                };

                const total_len = left_bytes.len + right_bytes.len;
                if (total_len <= merge_buffer.len) {
                    @memcpy(merge_buffer[0..left_bytes.len], left_bytes);
                    @memcpy(merge_buffer[left_bytes.len..total_len], right_bytes);

                    if (self.vocab.get(merge_buffer[0..total_len])) |merged_rank| {
                        if (merged_rank < best_rank) {
                            best_rank = merged_rank;
                            best_new_token = merged_rank;
                            best_pos = i;
                        }
                    }
                }

                i = next;
            }

            if (best_rank == std.math.maxInt(u32)) break;

            // Apply merge: replace left with merged, mark right as skipped
            tokens[best_pos] = best_new_token;

            // Find next active and mark as merged
            var next_active = best_pos + 1;
            while (next_active < tokens.len and skip[next_active] >= 0) : (next_active += 1) {}
            if (next_active < tokens.len) {
                skip[next_active] = @intCast(next_active);
                active_count -= 1;
            }
        }

        // Compact: collect only active tokens
        var write_pos: usize = 0;
        for (tokens, 0..) |token, i| {
            if (skip[i] < 0) {
                tokens[write_pos] = token;
                write_pos += 1;
            }
        }

        return write_pos;
    }

    fn applyMergesHashMapArrayList(self: *Tokenizer, tokens: *std.ArrayList(u32)) !void {
        var merge_buffer: [512]u8 = undefined;

        while (true) {
            var best_rank: u32 = std.math.maxInt(u32);
            var best_new_token: u32 = 0;
            var best_pos: usize = 0;

            var i: usize = 0;
            while (i + 1 < tokens.items.len) : (i += 1) {
                const left_token = tokens.items[i];
                const right_token = tokens.items[i + 1];

                const left_bytes = self.vocab_r.get(left_token) orelse continue;
                const right_bytes = self.vocab_r.get(right_token) orelse continue;

                const total_len = left_bytes.len + right_bytes.len;
                if (total_len > merge_buffer.len) continue;

                @memcpy(merge_buffer[0..left_bytes.len], left_bytes);
                @memcpy(merge_buffer[left_bytes.len..total_len], right_bytes);

                if (self.vocab.get(merge_buffer[0..total_len])) |merged_rank| {
                    if (merged_rank < best_rank) {
                        best_rank = merged_rank;
                        best_new_token = merged_rank;
                        best_pos = i;
                    }
                }
            }

            if (best_rank == std.math.maxInt(u32)) break;

            tokens.items[best_pos] = best_new_token;
            _ = tokens.orderedRemove(best_pos + 1);
        }
    }

    /// Encode text to token IDs
    ///
    /// IMPORTANT: Returned slice is valid until next encode() call
    /// or until tokenizer.deinit(). Do not free the returned slice.
    ///
    /// Example:
    ///   const tokens1 = try tokenizer.encode("hello");
    ///   // use tokens1...
    ///   const tokens2 = try tokenizer.encode("world");
    ///   // tokens1 now invalid! Use tokens2
    ///
    /// Implementation uses arena allocator for zero-cost returns on cache hits.
    /// Trie-based longest-match encoding (fast + correct).
    /// Falls back to HashMap if trie not available (WASM).
    pub fn encode(self: *Tokenizer, text: []const u8) ![]u32 {
        @setRuntimeSafety(false);

        // Reset arena (keeps capacity, fast O(1) operation)
        _ = self.encode_arena.reset(.retain_capacity);
        const arena = self.encode_arena.allocator();

        // Check full encoding cache first (for text < 1024 bytes)
        const should_cache = text.len < 1024;
        if (should_cache) {
            var encode_cache = getEncodeCache(self.allocator);
            if (encode_cache.get(text)) |cached_tokens| {
                // Cache hit! Allocate from arena (fast, bulk-freed on next encode)
                const result = try arena.alloc(u32, cached_tokens.len);
                @memcpy(result, cached_tokens);
                return result;
            }
        }

        // Cache miss - perform encoding
        var result = std.ArrayList(u32){};

        // Larger pre-allocation (2x for safety margin)
        try result.ensureTotalCapacity(arena, text.len * 2);

        // Iterate through chunks (zero allocations for splitting!)
        var chunk_iter = cl100k_splitter.chunks(text);
        while (chunk_iter.next()) |chunk| {
            const chunk_tokens = try self.encodeViaBacktrackingArena(chunk, arena);
            try result.appendSlice(arena, chunk_tokens);
        }

        // Return arena-allocated slice
        const items = result.items[0..result.items.len];
        const owned = try arena.dupe(u32, items);

        // Cache result before returning (if small enough)
        if (should_cache and owned.len < 256) {
            var encode_cache = getEncodeCache(self.allocator);
            const cached_text = try self.allocator.dupe(u8, text);
            const cached_tokens = try self.allocator.dupe(u32, owned);
            encode_cache.put(cached_text, cached_tokens) catch {}; // Ignore cache errors
        }

        return owned;
    }

    /// ZERO-ALLOCATION stack-based encoding with comptime specialization
    /// Uses size-specialized stack encoders to eliminate malloc/free overhead
    fn encodeViaBacktrackingArena(self: *Tokenizer, text: []const u8, arena: Allocator) ![]u32 {
        if (text.len == 0) return try arena.alloc(u32, 0);

        // Check LRU cache first (3-5x speedup for common chunks)
        // Only cache small chunks (< 1024 bytes) to avoid memory bloat
        const should_cache = text.len < 1024;
        if (should_cache) {
            var token_cache = getTokenCache(self.allocator);
            if (token_cache.get(text)) |cached_tokens| {
                // Cache hit! Return arena copy
                const result = try arena.alloc(u32, cached_tokens.len);
                @memcpy(result, cached_tokens);
                return result;
            }
        }

        // Cache miss or too large - encode and potentially cache result
        var tokens: []u32 = undefined;

        // Use Aho-Corasick + split_table + pair_lookup if available
        if (self.aho_corasick) |*ac| {
            @setRuntimeSafety(false);

            // Runtime dispatch to comptime-optimized stack encoders
            tokens = switch (text.len) {
                0...4096 => blk: {
                    var enc = try SmallEncoder.init(
                        text,
                        ac,
                        &self.vocab_r,
                        self.split_table,
                        @ptrCast(&self.merges_map),
                        self.next_prefix_match,
                    );
                    break :blk try enc.encode(arena);
                },
                4097...16384 => blk: {
                    var enc = try MediumEncoder.init(
                        text,
                        ac,
                        &self.vocab_r,
                        self.split_table,
                        @ptrCast(&self.merges_map),
                        self.next_prefix_match,
                    );
                    break :blk try enc.encode(arena);
                },
                16385...65536 => blk: {
                    var enc = try LargeEncoder.init(
                        text,
                        ac,
                        &self.vocab_r,
                        self.split_table,
                        @ptrCast(&self.merges_map),
                        self.next_prefix_match,
                    );
                    break :blk try enc.encode(arena);
                },
                else => blk: {
                    // Fall back to heap-based encoder for huge chunks (rare)
                    var encoder = try BacktrackEncoder.init(
                        arena,
                        text,
                        ac,
                        &self.vocab_r,
                        self.split_table,
                        @ptrCast(&self.merges_map),
                        self.next_prefix_match,
                    );
                    defer encoder.deinit();
                    break :blk try encoder.encode();
                },
            };
        } else {
            // Fallback: HashMap encoder (slow but works without Aho-Corasick)
            tokens = try self.encodeHashMap(text);
        }

        // Cache result if small enough (avoid caching large chunks)
        if (should_cache and tokens.len < 256) {
            var token_cache = getTokenCache(self.allocator);
            // Allocate persistent copies for cache
            const cached_text = try self.allocator.dupe(u8, text);
            const cached_tokens = try self.allocator.dupe(u32, tokens);
            token_cache.put(cached_text, cached_tokens) catch {}; // Ignore cache put errors
        }

        return tokens;
    }

    /// Decode token IDs back to text
    pub fn decode(self: *Tokenizer, tokens: []const u32) ![]u8 {
        return tokenizer_io.decode(self, tokens);
    }

    /// Save tokenizer to JSON file (HuggingFace-compatible format)
    pub fn saveToFile(self: *const Tokenizer, path: []const u8) !void {
        return tokenizer_io.saveToFile(self, path);
    }
};

// Tests
test "SIMD pair counting" {
    const ids = [_]u32{ 1, 2, 3, 2, 3, 2, 3, 4 };
    const pair = Pair{ .left = 2, .right = 3 };
    const count = countPairsSIMD(&ids, pair);
    try std.testing.expectEqual(@as(u32, 3), count);
}

test "pair merging" {
    var arena = std.heap.ArenaAllocator.init(std.testing.allocator);
    defer arena.deinit();
    const allocator = arena.allocator();

    var tokens = std.ArrayList(u32){};
    try tokens.appendSlice(allocator, &[_]u32{ 1, 2, 3, 2, 3, 4 });

    const pair = Pair{ .left = 2, .right = 3 };
    try mergePair(&tokens, pair, 100, allocator);

    try std.testing.expectEqualSlices(u32, &[_]u32{ 1, 100, 100, 4 }, tokens.items);
}
