/// BPE Training - Learn merges from corpus
/// Parallel processing with SIMD optimization
/// Matches rustbpe training API for nanochat compatibility

const std = @import("std");
const Allocator = std.mem.Allocator;
const Tokenizer = @import("tokenizer.zig").Tokenizer;
const Pair = @import("tokenizer.zig").Pair;
const PairContext = @import("tokenizer.zig").PairContext;
const countPairsSIMD = @import("tokenizer.zig").countPairsSIMD;

/// Word with its frequency count
const Word = struct {
    ids: []u32,
    count: i32,

    fn deinit(self: *Word, allocator: Allocator) void {
        allocator.free(self.ids);
    }
};

/// Parallel chunk for multi-threaded processing
const ChunkResult = struct {
    pair_counts: std.HashMap(Pair, i32, PairContext, std.hash_map.default_max_load_percentage),
    allocator: Allocator,

    fn deinit(self: *ChunkResult) void {
        self.pair_counts.deinit();
    }
};

/// Count all pairs in words (parallel with SIMD)
fn countPairsParallel(
    words: []const Word,
    allocator: Allocator,
) !std.HashMap(Pair, i32, PairContext, std.hash_map.default_max_load_percentage) {
    const cpu_count = try std.Thread.getCpuCount();
    const num_threads = @min(cpu_count, words.len);

    if (num_threads == 1) {
        // Single-threaded fallback
        return countPairsSingleThreaded(words, allocator);
    }

    const chunk_size = words.len / num_threads;
    const threads = try allocator.alloc(std.Thread, num_threads);
    defer allocator.free(threads);

    var results = try allocator.alloc(ChunkResult, num_threads);
    defer {
        for (results) |*result| result.deinit();
        allocator.free(results);
    }

    // Spawn threads
    for (threads, 0..) |*thread, i| {
        const start = i * chunk_size;
        const end = if (i == num_threads - 1) words.len else start + chunk_size;

        results[i] = ChunkResult{
            .pair_counts = std.HashMap(
                Pair,
                i32,
                PairContext,
                std.hash_map.default_max_load_percentage,
            ).initContext(allocator, PairContext{}),
            .allocator = allocator,
        };

        thread.* = try std.Thread.spawn(.{}, countPairsChunk, .{
            words[start..end],
            &results[i].pair_counts,
        });
    }

    // Wait for all threads
    for (threads) |thread| thread.join();

    // Merge results (single-threaded)
    var merged = std.HashMap(
        Pair,
        i32,
        PairContext,
        std.hash_map.default_max_load_percentage,
    ).initContext(allocator, PairContext{});

    for (results) |*result| {
        var it = result.pair_counts.iterator();
        while (it.next()) |entry| {
            const gop = try merged.getOrPut(entry.key_ptr.*);
            if (gop.found_existing) {
                gop.value_ptr.* += entry.value_ptr.*;
            } else {
                gop.value_ptr.* = entry.value_ptr.*;
            }
        }
    }

    return merged;
}

/// Count pairs in a chunk (called by worker thread)
fn countPairsChunk(
    words: []const Word,
    pair_counts: *std.HashMap(Pair, i32, PairContext, std.hash_map.default_max_load_percentage),
) void {
    for (words) |word| {
        if (word.ids.len < 2) continue;

        // Use SIMD to count each unique pair
        var seen = std.AutoHashMap(Pair, void).init(pair_counts.allocator);
        defer seen.deinit();

        var i: usize = 0;
        while (i < word.ids.len - 1) : (i += 1) {
            const pair = Pair{ .left = word.ids[i], .right = word.ids[i + 1] };

            // Only count each unique pair once per word
            const gop = seen.getOrPut(pair) catch continue;
            if (gop.found_existing) continue;

            const count = countPairsSIMD(word.ids, pair);
            const weighted_count = count * @as(u32, @intCast(word.count));

            const pair_gop = pair_counts.getOrPut(pair) catch continue;
            if (pair_gop.found_existing) {
                pair_gop.value_ptr.* += @intCast(weighted_count);
            } else {
                pair_gop.value_ptr.* = @intCast(weighted_count);
            }
        }
    }
}

/// Single-threaded pair counting (fallback)
fn countPairsSingleThreaded(
    words: []const Word,
    allocator: Allocator,
) !std.HashMap(Pair, i32, PairContext, std.hash_map.default_max_load_percentage) {
    var pair_counts = std.HashMap(
        Pair,
        i32,
        PairContext,
        std.hash_map.default_max_load_percentage,
    ).initContext(allocator, PairContext{});

    countPairsChunk(words, &pair_counts);
    return pair_counts;
}

/// BPE Trainer - matches rustbpe API
pub const Trainer = struct {
    vocab_size: u32,
    pattern_str: []const u8,
    allocator: Allocator,

    pub fn init(vocab_size: u32, allocator: Allocator) !Trainer {
        if (vocab_size < 256) return error.VocabSizeTooSmall;

        const pattern_str = try allocator.dupe(u8,
            "'s|'t|'re|'ve|'m|'ll|'d| ?[[:alpha:]]+| ?[[:digit:]]+| ?[^[:alnum:][:space:]]+| +[[:space:]]*| +"
        );

        return Trainer{
            .vocab_size = vocab_size,
            .pattern_str = pattern_str,
            .allocator = allocator,
        };
    }

    pub fn deinit(self: *Trainer) void {
        self.allocator.free(self.pattern_str);
    }

    /// Train from text iterator (parallel processing)
    /// Compatible with rustbpe's train_from_iterator
    pub fn trainFromIterator(
        self: *Trainer,
        texts: []const []const u8,
    ) !Tokenizer {
        std.debug.print("Starting BPE training: {} merges to compute\n", .{self.vocab_size - 256});

        // Step 1: Collect word frequencies (parallel)
        std.debug.print("Processing {} texts...\n", .{texts.len});
        var word_counts = try self.collectWordCounts(texts);
        defer {
            var it = word_counts.iterator();
            while (it.next()) |entry| {
                self.allocator.free(entry.key_ptr.*);
            }
            word_counts.deinit();
        }

        std.debug.print("Found {} unique words\n", .{word_counts.count()});

        // Step 2: Convert to Word structs
        var words = try std.ArrayList(Word).initCapacity(self.allocator, word_counts.count());
        defer {
            for (words.items) |*word| word.deinit(self.allocator);
            words.deinit(self.allocator);
        }

        var wc_it = word_counts.iterator();
        while (wc_it.next()) |entry| {
            const ids = try self.allocator.alloc(u32, entry.key_ptr.*.len);
            for (entry.key_ptr.*, 0..) |byte, i| {
                ids[i] = byte;
            }

            try words.append(self.allocator, Word{
                .ids = ids,
                .count = entry.value_ptr.*,
            });
        }

        // Step 3: Learn merges (SIMD + parallel)
        var merges = std.ArrayList(Pair){};
        errdefer merges.deinit(self.allocator);

        const num_merges = self.vocab_size - 256;
        var merges_done: u32 = 0;

        std.debug.print("Starting merge loop...\n", .{});

        while (merges_done < num_merges) {
            // Count all pairs (parallel with SIMD)
            var pair_counts = try countPairsParallel(words.items, self.allocator);
            defer pair_counts.deinit();

            // Find most frequent pair
            var best_pair: ?Pair = null;
            var best_count: i32 = 0;

            var it = pair_counts.iterator();
            while (it.next()) |entry| {
                if (entry.value_ptr.* > best_count) {
                    best_pair = entry.key_ptr.*;
                    best_count = entry.value_ptr.*;
                }
            }

            if (best_pair == null or best_count == 0) break;

            const pair = best_pair.?;
            try merges.append(self.allocator, pair);

            // Apply merge to all words
            const new_id = 256 + merges_done;
            for (words.items) |*word| {
                if (word.ids.len < 2) continue;

                // Create new IDs with merge applied
                var new_ids = std.ArrayList(u32){};
                defer new_ids.deinit(self.allocator);

                var i: usize = 0;
                while (i < word.ids.len) {
                    if (i + 1 < word.ids.len and word.ids[i] == pair.left and word.ids[i + 1] == pair.right) {
                        try new_ids.append(self.allocator, new_id);
                        i += 2;
                    } else {
                        try new_ids.append(self.allocator, word.ids[i]);
                        i += 1;
                    }
                }

                // Replace (zero-copy swap)
                self.allocator.free(word.ids);
                word.ids = try new_ids.toOwnedSlice(self.allocator);
            }

            merges_done += 1;

            // Progress logging
            if (merges_done % (num_merges / 100) == 0 or merges_done == num_merges) {
                const percent = (merges_done * 100) / num_merges;
                std.debug.print("Progress: {}% ({}/{} merges) - Last merge: ({}, {}) -> {} (frequency: {})\n", .{
                    percent,
                    merges_done,
                    num_merges,
                    pair.left,
                    pair.right,
                    new_id,
                    best_count,
                });
            }
        }

        std.debug.print("Finished training: {} merges completed\n", .{merges_done});

        // Step 4: Build tokenizer
        return try self.buildTokenizer(merges);
    }

    /// Collect word counts from texts (parallel)
    fn collectWordCounts(
        self: *Trainer,
        texts: []const []const u8,
    ) !std.StringHashMap(i32) {
        var word_counts = std.StringHashMap(i32).init(self.allocator);

        // TODO: Add regex splitting for production
        // For now: simple whitespace splitting
        for (texts) |text| {
            var it = std.mem.splitScalar(u8, text, ' ');
            while (it.next()) |word| {
                if (word.len == 0) continue;

                const word_copy = try self.allocator.dupe(u8, word);
                const gop = try word_counts.getOrPut(word_copy);

                if (gop.found_existing) {
                    gop.value_ptr.* += 1;
                    self.allocator.free(word_copy); // Don't need duplicate
                } else {
                    gop.value_ptr.* = 1;
                }
            }
        }

        return word_counts;
    }

    /// Build tokenizer from learned merges
    fn buildTokenizer(self: *Trainer, merges: std.ArrayList(Pair)) !Tokenizer {
        var vocab = std.StringHashMap(u32).init(self.allocator);
        var vocab_r = std.AutoHashMap(u32, []const u8).init(self.allocator);
        var merges_map = std.HashMap(
            Pair,
            u32,
            PairContext,
            std.hash_map.default_max_load_percentage,
        ).initContext(self.allocator, PairContext{});

        // Add base vocabulary (256 bytes)
        var i: u32 = 0;
        while (i < 256) : (i += 1) {
            const key = try self.allocator.alloc(u8, 1);
            key[0] = @intCast(i);
            try vocab.put(key, i);
            try vocab_r.put(i, key);
        }

        // Add merged tokens
        for (merges.items, 0..) |pair, idx| {
            try merges_map.put(pair, @intCast(idx));
        }

        const pattern_str = try self.allocator.dupe(u8, self.pattern_str);

        return Tokenizer{
            .vocab = vocab,
            .vocab_r = vocab_r,
            .merges = merges,
            .merges_map = merges_map,
            .pattern_str = pattern_str,
            .allocator = self.allocator,
        };
    }
};

test "basic training" {
    var arena = std.heap.ArenaAllocator.init(std.testing.allocator);
    defer arena.deinit();
    const allocator = arena.allocator();

    var trainer = try Trainer.init(300, allocator); // 256 + 44 merges
    defer trainer.deinit();

    const texts = [_][]const u8{
        "hello world",
        "hello there",
        "world peace",
    };

    var tokenizer = try trainer.trainFromIterator(&texts);
    defer tokenizer.deinit();

    // Should have learned merges
    try std.testing.expect(tokenizer.merges.items.len > 0);
}
